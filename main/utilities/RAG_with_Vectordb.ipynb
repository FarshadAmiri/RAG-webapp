{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import accelerate\n",
    "import torch\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring text generation model, tokenizer, computational device and optional streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting device\n",
    "gpu=0\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name and hf token\n",
    "name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "# name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "\n",
    "# hugginf face auth token\n",
    "# file_path = \"../../huggingface_credentials.txt\"\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     auth_token = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name\n",
    "    # ,cache_dir='./model/'\n",
    "    # ,use_auth_token=auth_token\n",
    "    ,device_map='cuda'                 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45178f0d7b594cb2b158cca2af07aadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: D:\\NLP 1\\venv\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary D:\\NLP 1\\venv\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = AutoModelForCausalLM.from_pretrained(name\n",
    "    ,cache_dir=r\"C:\\Users\\user2\\.cache\\huggingface\\hub\"\n",
    "    # ,cache_dir='./model/'\n",
    "    # ,use_auth_token=auth_token\n",
    "    ,device_map='cuda'  \n",
    "    # , torch_dtype=torch.float16\n",
    "    # ,low_cpu_mem_usage=True\n",
    "    # ,rope_scaling={\"type\": \"dynamic\", \"factor\": 2}\n",
    "    # ,load_in_8bit=True,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_inference(plain_text, model, tokenizer, device, streamer=None, max_length=4000, ):\n",
    "    input_ids = tokenizer(\n",
    "        plain_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        )['input_ids'].to(device)\n",
    "    \n",
    "    output_ids = model.generate(input_ids\n",
    "                        ,streamer=streamer\n",
    "                        ,use_cache=True\n",
    "                        ,max_new_tokens=float('inf')\n",
    "                       )\n",
    "    answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating texts using a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "To train a machine learning model, you typically follow these steps:\n",
      "1. Collect and preprocess data.\n",
      "2. Choose a machine learning algorithm.\n",
      "3. Split the data into training and validation sets.\n",
      "4. Train the model on the training set.\n",
      "5. Evaluate the model's performance on the validation set.\n",
      "6. Fine-tune the model as needed.\n",
      "7. Test the final model on new data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"what are the steps to train a machine learning model? explain in less than 100 words.\\nTo train a machine learning model, you typically follow these steps:\\n1. Collect and preprocess data.\\n2. Choose a machine learning algorithm.\\n3. Split the data into training and validation sets.\\n4. Train the model on the training set.\\n5. Evaluate the model's performance on the validation set.\\n6. Fine-tune the model as needed.\\n7. Test the final model on new data.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what are the steps to train a machine learning model? explain in less than 100 words\"\n",
    "res = llm_inference(text, model, tokenizer, device, streamer=streamer,)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma db\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"../../vdb\")\n",
    "\n",
    "# get collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system prompt\n",
    "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as\n",
    "helpfully as possible, while being safe.\n",
    "If a question does not make any sense, or is not factually coherent, explain\n",
    "why instead of answering something not correct. If you don't know the answer\n",
    "to a question, please don't share false information.\n",
    "Try to be exact in information and numbers you tell.\n",
    "Your goal is to provide answers based on the information provided and your other knowledge.<</SYS>>\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(context_window=4096,\n",
    "                     max_new_tokens=512,\n",
    "                     system_prompt=system_prompt,\n",
    "                     query_wrapper_prompt=query_wrapper_prompt,\n",
    "                     model=model,\n",
    "                     tokenizer=tokenizer)\n",
    "\n",
    "embeddings = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    "    llm=llm,\n",
    "    embed_model=embeddings\n",
    ")\n",
    "\n",
    "# And set the service context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert a single document into the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "loader = PyMuPDFReader()\n",
    "\n",
    "# Load documents\n",
    "doc_dir = r\"D:\\NLP 1\\RAG-webapp\\documents_db\\Sattelite imagery article scripts.pdf\"\n",
    "document = loader.load(file_path=Path(doc_dir), metadata=False)\n",
    "\n",
    "# Create indexes\n",
    "# for doc in document:\n",
    "#     index.insert(doc, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert directory of documents into the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some documents\n",
    "documents = SimpleDirectoryReader(r\"D:\\NLP 1\\RAG-webapp\\documents_db\").load_data()\n",
    "\n",
    "# create your index\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    "    doc_id = [\"fe0ab12b-146f-45e2-9161-387ac90f8031\"]\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(streaming=True)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.2)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided in the PDF, the person who studied Master of Science in Management with a background in Civil Engineering is Farshad Amiri.</s>"
     ]
    }
   ],
   "source": [
    "# create a query engine and query\n",
    "response = query_engine.query(\"who studied Master of Science in Management with a background in Civil Engineering?\")\n",
    "# response = query_engine.query(\"how many gold medals Iranian youth won in 2023 chess competitions?\")\n",
    "# response = query_engine.query(\"describe key points of 2023 climate change?\")\n",
    "# response = query_engine.query(\"how much headline inflation increased after storm shock?\")\n",
    "# response = query_engine.query(\"how many gold medals Iranian youth won in 2023 chess competitions?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23824048972736303\n",
      "0.21386191409572436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='20ff3158-cfd4-4063-b656-0e8f5b5ac693', embedding=None, metadata={'page_label': '1', 'file_name': 'Iran medals in Asia competitions 2023.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5502d6ee-b6f1-4dd5-8406-26e96215d078', node_type=None, metadata={'page_label': '1', 'file_name': 'Iran medals in Asia competitions 2023.pdf'}, hash='3de8a565a78341bb04481d2b423e39bf0923e55979253e643aa0a340d9d04135')}, hash='51e80e575f1121004378dcc6fb6fc45dbb08e7462a1d7fc3c1111e25ecd6eb23', text='Iran wins six medals in 2023 Asian Youth \\nChess Championships  \\n \\nTEHRAN – The Iranian chess players claimed six medals at the 25th Asian Youth Chess \\nChampionships.  \\nRamtin Kakavand won two gold medals in the boys’  under 10 standard and blitz.  \\nRosha Akbari claimed a bronze in the girls’ under 12 blitz.  \\nMohammadtaha   Arkak seized a bronze in the boys’ under 8 blitz.  \\nKanaan Pourmousavi took a bronze in the boys’ under 12 blitz.  \\nNiyousha Mohammadi also won a bronze medal in the girls’ under 14 standard.  \\nThe competition was held at the Al Ain Chess Club in the UAE from Dec. 12 to 22.', start_char_idx=0, end_char_idx=600, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.23824048972736303)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    print(node.score)\n",
    "response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
