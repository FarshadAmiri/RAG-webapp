{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import accelerate\n",
    "import torch\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring text generation model, tokenizer, computational device and optional streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device\n",
    "gpu=0\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name and hf token\n",
    "name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "# name = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "\n",
    "# hugginf face auth token\n",
    "file_path = \"../../huggingface_credentials.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    auth_token = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name\n",
    "    # ,cache_dir='./model/'\n",
    "    ,use_auth_token=auth_token\n",
    "    ,device_map='cuda'                 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = AutoModelForCausalLM.from_pretrained(name\n",
    "    ,cache_dir=r\"C:\\Users\\user2\\.cache\\huggingface\\hub\"\n",
    "    # ,cache_dir='./model/'\n",
    "    ,use_auth_token=auth_token\n",
    "    ,device_map='cuda'  \n",
    "    # , torch_dtype=torch.float16\n",
    "    # ,low_cpu_mem_usage=True\n",
    "    # ,rope_scaling={\"type\": \"dynamic\", \"factor\": 2}\n",
    "    # ,load_in_8bit=True,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_inference(plain_text, model, tokenizer, device, streamer=None, max_length=4000, ):\n",
    "    input_ids = tokenizer(\n",
    "        plain_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        )['input_ids'].to(device)\n",
    "    \n",
    "    output_ids = model.generate(input_ids\n",
    "                        ,streamer=streamer\n",
    "                        ,use_cache=True\n",
    "                        ,max_new_tokens=float('inf')\n",
    "                       )\n",
    "    answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating texts using a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"what are the steps to train a machine learning model? explain in less than 100 words\"\n",
    "res = llm_inference(text, model, tokenizer, device, streamer=streamer,)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
