{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"../../vdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system prompt\n",
    "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as\n",
    "helpfully as possible, while being safe. Your answers should not include\n",
    "any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain\n",
    "why instead of answering something not correct. If you don't know the answer\n",
    "to a question, please don't share false information.\n",
    "\n",
    "Your goal is to provide answers based on the information provided and your other knowledges.<</SYS>>\n",
    "\"\"\"\n",
    "# Throw together the query wrapper\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")\n",
    "\n",
    "llm = HuggingFaceLLM(context_window=4096,\n",
    "                    max_new_tokens=512,\n",
    "                    system_prompt=system_prompt,\n",
    "                    query_wrapper_prompt=query_wrapper_prompt,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\NLP 1\\RAG-webapp\\main\\utilities\\draft_1.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/draft_1.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load your index from stored vectors\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/draft_1.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m index \u001b[39m=\u001b[39m VectorStoreIndex\u001b[39m.\u001b[39;49mfrom_vector_store(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/draft_1.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     vector_store, storage_context\u001b[39m=\u001b[39;49mstorage_context\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/draft_1.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "File \u001b[1;32mD:\\NLP 1\\venv\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:68\u001b[0m, in \u001b[0;36mVectorStoreIndex.from_vector_store\u001b[1;34m(cls, vector_store, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     64\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot initialize from a vector store that does not store text.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m     )\n\u001b[0;32m     67\u001b[0m storage_context \u001b[39m=\u001b[39m StorageContext\u001b[39m.\u001b[39mfrom_defaults(vector_store\u001b[39m=\u001b[39mvector_store)\n\u001b[1;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m     69\u001b[0m     nodes\u001b[39m=\u001b[39;49m[], service_context\u001b[39m=\u001b[39;49mservice_context, storage_context\u001b[39m=\u001b[39;49mstorage_context\n\u001b[0;32m     70\u001b[0m )\n",
      "File \u001b[1;32mD:\\NLP 1\\venv\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:46\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async \u001b[39m=\u001b[39m use_async\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_nodes_override \u001b[39m=\u001b[39m store_nodes_override\n\u001b[1;32m---> 46\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     47\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[0;32m     48\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[0;32m     49\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[0;32m     50\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[0;32m     51\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[0;32m     52\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m     53\u001b[0m )\n",
      "File \u001b[1;32mD:\\NLP 1\\venv\\Lib\\site-packages\\llama_index\\indices\\base.py:61\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnodes must be a list of Node objects.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context \u001b[39m=\u001b[39m service_context \u001b[39mor\u001b[39;00m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults()\n\u001b[0;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context \u001b[39m=\u001b[39m storage_context \u001b[39mor\u001b[39;00m StorageContext\u001b[39m.\u001b[39mfrom_defaults()\n\u001b[0;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docstore \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mdocstore\n",
      "File \u001b[1;32mD:\\NLP 1\\venv\\Lib\\site-packages\\llama_index\\indices\\service_context.py:159\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[1;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot specify both llm and llm_predictor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m     llm_predictor \u001b[39m=\u001b[39m LLMPredictor(llm\u001b[39m=\u001b[39mllm)\n\u001b[1;32m--> 159\u001b[0m llm_predictor \u001b[39m=\u001b[39m llm_predictor \u001b[39mor\u001b[39;00m LLMPredictor()\n\u001b[0;32m    160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm_predictor, LLMPredictor):\n\u001b[0;32m    161\u001b[0m     llm_predictor\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n",
      "File \u001b[1;32mD:\\NLP 1\\venv\\Lib\\site-packages\\llama_index\\llm_predictor\\base.py:66\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[1;34m(self, llm, callback_manager)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     62\u001b[0m     llm: Optional[LLMType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     63\u001b[0m     callback_manager: Optional[CallbackManager] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     64\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39m=\u001b[39m resolve_llm(llm)\n\u001b[0;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m callback_manager:\n\u001b[0;32m     69\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n",
      "File \u001b[1;32mD:\\NLP 1\\venv\\Lib\\site-packages\\llama_index\\llms\\utils.py:17\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm, BaseLanguageModel):\n\u001b[0;32m     14\u001b[0m     \u001b[39m# NOTE: if it's a langchain model, wrap it in a LangChainLLM\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m LangChainLLM(llm\u001b[39m=\u001b[39mllm)\n\u001b[1;32m---> 17\u001b[0m \u001b[39mreturn\u001b[39;00m llm \u001b[39mor\u001b[39;00m OpenAI()\n",
      "File \u001b[1;32mD:\\NLP 1\\venv\\Lib\\site-packages\\llama_index\\llms\\openai.py:50\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, model, temperature, max_tokens, additional_kwargs, max_retries, callback_manager, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     41\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     42\u001b[0m     model: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-davinci-003\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m     49\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     validate_openai_api_key(\n\u001b[0;32m     51\u001b[0m         kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mapi_key\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mapi_type\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m     55\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemperature \u001b[39m=\u001b[39m temperature\n",
      "File \u001b[1;32mD:\\NLP 1\\venv\\Lib\\site-packages\\llama_index\\llms\\openai_utils.py:268\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key, api_type)\u001b[0m\n\u001b[0;32m    263\u001b[0m openai_api_type \u001b[39m=\u001b[39m (\n\u001b[0;32m    264\u001b[0m     api_type \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_TYPE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_type\n\u001b[0;32m    265\u001b[0m )\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n\u001b[0;32m    269\u001b[0m \u001b[39melif\u001b[39;00m openai_api_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mopen_ai\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m OPENAI_API_KEY_FORMAT\u001b[39m.\u001b[39msearch(\n\u001b[0;32m    270\u001b[0m     openai_api_key\n\u001b[0;32m    271\u001b[0m ):\n\u001b[0;32m    272\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(INVALID_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n"
     ]
    }
   ],
   "source": [
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "# create your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the meaning of life?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
